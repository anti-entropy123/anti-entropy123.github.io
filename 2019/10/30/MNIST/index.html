<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <link rel="canonical" href="https://tjuyjn.top/2019/10/30/MNIST/">
    
    
    <title>机器学习入门 | Anti-entropy&#39;s blog | 向一切虚伪的美好宣战</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="tensorflow,深度学习,GAN">
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.css">
    <link rel="stylesheet" href="/css/style.css?v=1.4.2">
    
    <script type="text/javascript">
        // Data Center
        var DC = {
            reward:	true,
            lv: JSON.parse('{"enable":null,"app_id":"falsee1f779yetMaV1lNERajntYJH-gzGzoHsz","app_key":"CGROdujMx4Hw9l84Eysyhp7D","icon":true}'),
            v: JSON.parse('{"enable":true,"appid":"w6aWfPRvBTtEbBcUQVnWa1fl-MdYXbMMI","appkey":"96PaRwN0DWkNUdwQrWx3Ff7g","notify":true,"verify":true,"placeholder":"来和希儿一起玩吧 -- 记得告诉希儿你的昵称和邮箱哟","avatar":"retro"}'),
            g: JSON.parse('{"enable":false,"lazy":true,"owner":"codefine","repo":"gitment","oauth":{"client_id":null,"client_secret":null},"perPage":10}'),
            d: JSON.parse('{"app_id":null}')
        };
    </script>
    <script type="text/javascript">
        window.lazyScripts=[];
    </script>
    

    <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?85ac030db207eb5660d0fd7b00b2205d";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
        <!-- live2d css -->
        <style>
            .live2d-widget-dialog-container {
                width: 300px;
                height: 120px;
                position: absolute;
                bottom: 65%;
                right: 0px;
                transform-origin: right;
                padding: 12px;
                box-sizing: border-box;
                -webkit-font-smoothing: antialiased;
            }
            .live2d-widget-dialog {
                width: 100%;
                height: 100%;
                color: #917159;
                font-size: 16px;
                padding: 12px;
                border: 2px solid rgb(236, 203, 180);
                background: rgb(252, 248, 244);
                box-sizing: border-box;
                border-radius: 10px;
                transform: rotate(-2deg);
                opacity: 0;
                transition: 200ms opacity;
                box-shadow: rgba(0, 0, 0, 0.12) 0px 1px 6px, rgba(0, 0, 0, 0.12) 0px 1px 4px;
                animation: live2d-widget-dialog-tingle 4s ease-in-out 0s infinite alternate;
            }
            @keyframes live2d-widget-dialog-tingle {
                0% { transform: translate(-1px, 1.5px) rotate(-2deg); }
                100% { transform: translate(1px, -1.5px) rotate(2deg); }
            }
        </style>
    
        <!--- live2d js -->
        <script src="http://188.131.227.20:999/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script>
        <script>L2Dwidget.init({"pluginRootPath":"http://188.131.227.20:999/live2dw/","pluginJsPath":"http://188.131.227.20:999/lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"position":"right","width":300,"height":400},"mobile":{"show":false},"log":false});</script>
        <div id="live2d-widget" class="live2d-widget-container" style="position: fixed; right: 0px; bottom: -20px; width: 300px; height: 400px; z-index: 99999; opacity: 1; pointer-events: none;"><canvas id="live2dcanvas" width="600" height="800" style="position: absolute; left: 0px; top: 0px; width: 300px; height: 400px;"></canvas></div>
        <script>
            var imageArr=[];
        </script>
        
</head>



<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap">
      
      <img src="/img/brand.jpg" class="brand-bg">
      
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Anti-entropy</h5>
          <a href="mailto:1348651580@qq.com" title="1348651580@qq.com" class="mail">
            
              <span>1</span>
            
              <span>3</span>
            
              <span>4</span>
            
              <span>8</span>
            
              <span>6</span>
            
              <span>5</span>
            
              <span>1</span>
            
              <span>5</span>
            
              <span>8</span>
            
              <span>0</span>
            
              <span>@</span>
            
              <span>q</span>
            
              <span>q</span>
            
              <span>.</span>
            
              <span>c</span>
            
              <span>o</span>
            
              <span>m</span>
            
          </a>
        </hgroup>
        
        <ul class="menu-link">
          
              <li>
                <a href="https://github.com/anti-entropy123/" target="_blank">
                  <i class="icon icon-lg icon-github"></i>
                </a>
              </li>
            
        </ul>
        
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
          <div id="binft" style=" text-align:center;">
              <script>
                var binft = function (r) {
                  function t() {
                    return b[Math.floor(Math.random() * b.length)]
                  }  
                  function e() {
                    return String.fromCharCode(94 * Math.random() + 33)
                  }
                  function n(r) {
                    for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {
                      var l = document.createElement("span");
                      l.textContent = e(), l.style.color = t(), n.appendChild(l)
                    }
                    return n
                  }
                  function i() {
                    var t = o[c.skillI];
                    c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)
                  }
                  var l = "",
                  o = ["孤独是常态,", "寂寞是病态",
                      "孤独使我自由,","寂寞让我作呕",
                      "为守护所有美好而战,","向一切虚伪的美好宣战",
                      "don't be sorry,","be better",
                      "伤痕是我的勋章",
                      "但置我于低谷者,","必将助我以崛起",
                      "但置我于死地者,","必将赐我以后生",
                      "行于黑暗, 侍奉光明","万物皆虚, 万事皆允",
                      "天生我材必有用,","千金散尽还复来"
                      ].map(function (r) {
                  return r + ""
                  }),
                  a = 2,
                  g = 1,
                  s = 5,
                  d = 75,
                  b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],
                  c = {
                    text: "",
                    prefixP: -s,
                    skillI: 0,
                    skillP: 0,
                    direction: "forward",
                    delay: a,
                    step: g
                  };
                  i()
                  };
                  binft(document.getElementById('binft'));
              </script>
            </div>
        
            <li class="">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                文章
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row clearfix">
        <a href="javascript:;" class="header-icon pull-left waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">
            <span>机器学习入门</span>
            
        </div>
        
        <a href="javascript:;" id="site_search_btn" class="header-icon pull-right waves-effect waves-circle waves-light">
            <i class="icon icon-lg icon-search"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">
    <img src="/img/banner.jpg" class="header-bg">
    <div class="container fade-scale">
        <h1 class="title">机器学习入门</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-10-30T10:57:26.000Z" itemprop="datePublished" class="page-time">
  2019-10-30
</time>


            
        </h5>
        
    </div>
    

</header>

<div id="site_search">
    <div class="search-title clearfix">
        <span class="pull-left">
          <i class="icon icon-lg icon-search"></i>
        </span>
        <input type="text" id="local-search-input" name="q" results="0" placeholder="search my blog..." class="form-control pull-left"/>
        <a href="javascript:;" class="close pull-right waves-effect waves-circle waves-light">
          <i class="icon icon-lg icon-close"></i>
        </a>
    </div>
    <div id="local-search-result"></div>
</div>


<div class="container body-wrap">
    <article id="post-MNIST"
  class="post-article article-type-post" itemprop="blogPost">
    <div class="post-card">
        <h1 class="post-card-title">机器学习入门</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-10-30 18:57:26" datetime="2019-10-30T10:57:26.000Z"  itemprop="datePublished">2019-10-30</time>

            


            

            
    <span class="leancloud-comment">
        <i class="icon icon-comment-o"></i>
        <a href="/2019/10/30/MNIST/#comment">
            <span class="valine-comment-count" data-xid="/2019/10/30/MNIST/"></span>
        </a>
    </span>



            
        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            
            <a id="more"></a>

<h1 id="MNIST神经网络"><a href="#MNIST神经网络" class="headerlink" title="MNIST神经网络"></a>MNIST神经网络</h1><p>主要参考 : <a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_beginners.html" target="_blank" rel="noopener">tensorflow 中文社区</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>MNIST是一个入门级的计算机视觉数据集，它包含各种手写数字图片:</p>
<figure class="image-box">
                <a rel=机器学习入门 href="http://www.tensorfly.cn/tfdoc/images/MNIST.png" title="" data-fancybox="images"><img src="http://www.tensorfly.cn/tfdoc/images/MNIST.png" alt title class></a>
                <p></p>
            </figure>

<p>在此教程中，我们将训练一个机器学习模型用于预测图片里面的数字.</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集的下载和安装本来可以用 tensorflow 社区提供的一份python代码来完成. 可惜链接是指向google 的网盘的, 我手上暂时也没可用的梯子, 只好百度了一份 <code>input_data.py</code>, 然而这份代码年代已经十分久远, 已经不适合现在版本的numpy了, 所以我又只能下载安装一份旧的numpy <code>pip install numpy==1.16.0</code>. But 又因为在国内连接<code>pypi</code>也十分的慢, socket会报 <code>time out</code> 错误, 我又只能换了下<code>douban</code>的镜像<code>pip install redis -i https://pypi.douban.com/simple</code>. 这下才算解决了数据集的问题(苦笑).</p>
<p>在使用中, 需要把<code>input_data.py</code>先<code>import</code>进 python 的主程序中, 如果数据集没有被提前下载, 那么在运行时会调用<code>input_data</code>中的函数下载数据集. 建议先提前下载好数据集放置在固定的目录下, 可以避免运行时下载速度太慢的问题.</p>
<p>实际使用中的数据集会被分为两部分, 60000行的训练数据集(mnist.train)和10000行的测试数据集(mnist.test). 有了这个单独用来测试而不会用于训练的数据集会使模型更容易推广到其他数据集上(泛化).</p>
<p>而每一组数据单元包含一个手写图片和一个对应的标签的数据. 所有图片的数据可以由一个形状为<code>[60000, 784]</code>的张量来表示, 第一个维度当做图片的索引. 所有标签的数据用张量<code>[60000,10]</code>来表示. 且此处的10维向量是独热的, 代表 0 - 9 十种可能.</p>
<h2 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h2><p>目标: 求得每个像素对于特定数字类的权值, 即得到一张给定图片属于某个特定数字类的证据(evidence), 证据值越大, 证明越有可能是这个数字类. </p>
<p>对于给定的输入图片 x 它代表的是数字 i 的证据可以表示为:<br>$$<br>evidence_i = \sum_j W_{i,j} + b_i<br>$$<br>其中$W_i$ 代表权重, $b_i$代表数字 i 类的偏置量, j 代表给定图片 x 的像素索引用于像素求和. 然后用softmax函数可以把这些证据转换成概率 y:<br>$$y = softmax(evidence)$$</p>
<p>可以看成是一个激励(activation)函数, 把我们定义的线性函数的输出转换成我们想要的格式, 也就是关于10个数字类的概率分布.</p>
<p>$$<br>\begin{aligned}<br>softmax(x) &amp;= normalize(exp(x)) \<br>softmax(x)_i &amp;= \cfrac{exp(x_i)}{\Sigma_jexp(x_j)}<br>\end{aligned}<br>$$</p>
<p>此处的<code>normalize()</code>为归一化, 归一化就是要把需要处理的数据经过处理后(通过某种算法)限制在你需要的一定范围内, 可以方便后面数据处理的方便, 其次是保证程序运行时的概率分布.</p>
<p>回归模型可以用下面的图解释:</p>
<figure class="image-box">
                <a rel=机器学习入门 href="http://www.tensorfly.cn/tfdoc/images/softmax-regression-scalargraph.png" title="" data-fancybox="images"><img src="http://www.tensorfly.cn/tfdoc/images/softmax-regression-scalargraph.png" alt title class></a>
                <p></p>
            </figure>
<figure class="image-box">
                <a rel=机器学习入门 href="http://www.tensorfly.cn/tfdoc/images/softmax-regression-scalarequation.png" title="" data-fancybox="images"><img src="http://www.tensorfly.cn/tfdoc/images/softmax-regression-scalarequation.png" alt title class></a>
                <p></p>
            </figure>
<figure class="image-box">
                <a rel=机器学习入门 href="http://www.tensorfly.cn/tfdoc/images/softmax-regression-vectorequation.png" title="" data-fancybox="images"><img src="http://www.tensorfly.cn/tfdoc/images/softmax-regression-vectorequation.png" alt title class></a>
                <p></p>
            </figure>

<p>第三张图的 W 矩阵即为权重值矩阵. 此处矩阵相乘的形式与下面 tf 的代码十分相似.</p>
<figure class="image-box">
                <a rel=机器学习入门 href="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/matrix1.png" title="" data-fancybox="images"><img src="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/matrix1.png" alt title class></a>
                <p></p>
            </figure>
<p>这里公式其实是单就一张图片而言. x1,x2,x3代表了 x 的第 1,2,3 个像素. 将矩阵乘法展开后就成了加权求和的形式. 不过在实际中, 图片是有 784 个像素的, 输出的向量元素个数也应该为10, 所以严格来说公式应该是这个样子.</p>
<figure class="image-box">
                <a rel=机器学习入门 href="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/matrix2.png" title="" data-fancybox="images"><img src="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/matrix2.png" alt title class></a>
                <p></p>
            </figure>
<p>目前为止这个模型的思想似乎很清晰了, 但是 <code>softmax 回归</code> 在这里究竟是什么作用还是有些不明朗, 不过我大致猜测其实 softmax 与模型参数 W 和 b 的确定并没有太大的关系, 它在这里主要的作用是将计算结果映射到 [0,1] 的区间上, 使结果作为概率值是有意义的. 在神经网络上, softmax 也是作用在最后一层. 综上, softmax 的作用是对模型输出的一层 “约束”.</p>
<h2 id="模型设定"><a href="#模型设定" class="headerlink" title="模型设定"></a>模型设定</h2><p>我所理解的模型设定这部分就是使用 tensorflow 的语言描述出数学模型的公式的过程. 这里所需要描述的公式如上图所示.</p>
<p>根据公式, 我们在代码中需要实现的有 tensorflow变量 x 和 y分别用来存储输入的数据和模型输出的数据, 并且还有 权重矩阵W 和 偏置量b.<br>还需要在程序中设定好使用的模型为 <code>softmax 回归</code>.</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>交叉熵(cross-entropy)成本函数.<br>$$<br>H_{y’}(y) = -\sum_i y^{‘}_ilog(y_i)<br>$$<br>其中 y 是预测的概率分布, y’ 是实际的分布(输入的 one-hot vector, 这里输入为独热的向量也是为了和模型的输出的形式相同.).</p>
<p>交叉熵代表了模型计算结果中的损失(loss), 也就是说交叉熵越大, 那么计算结果的误差也就越大.</p>
<p>代码中要求TensorFlow用梯度下降算法以 0.01 的学习速率最小化交叉熵. 使用随机梯度的下降训练的意义在于, 在每一步训练时不使用所有的数据, 而是选取其中的子集来节省计算的开销. 在每一步的计算中, tensorflow 会自动使用反向传播算法(backpropagation algorithm)来不断降低 loss 的值. 我在网上找到了反向传播算法的介绍, 其原理大致为 : 当输入的数据集相同时, 损失函数的值 C 可以看做是关于 权重矩阵W 和 偏置量b 的二元函数. 也就是 $C = f(W,b)$, 当 C 的值最小时, 周围的函数是平滑的(也有不是的情况), 也就相当于偏导数为 0.<br>$$<br>\begin{aligned}<br>&amp;\cfrac{\partial C}{\partial W} = 0 \<br>&amp;\cfrac{\partial C}{\partial b} = 0<br>\end{aligned}<br>$$<br>经过多轮迭代使参数 W 和 b 达到接近这样的状态时, 梯度下降的一步就完成了. 但是具体每步的步长是多少要根据学习速率和步长来计算.</p>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><p>模型评估的方法比较简单, 就是用 minst.test 中的数据, 将图片的数据输入到训练好的模型中, 然后调用 tf 中的函数与标签中的结果进行比对算出正确率. 这里的 test 数据应该与 train 中的数据不能有重合.</p>
<h2 id="完整代码和我的注释"><a href="#完整代码和我的注释" class="headerlink" title="完整代码和我的注释"></a>完整代码和我的注释</h2><p>input_data.py 可以在百度上搜到, 就不贴在这里了.</p>
<p>MNIST.py</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 input_data.py 中的代码导入需要的数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###  模型构建  ####</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>])) <span class="comment"># W 权重矩阵</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))     <span class="comment"># b 偏置量</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x,W)+b) <span class="comment"># y 输出值, 同时指出了模型为 softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练模型以及评估 ####</span></span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>,[<span class="literal">None</span>,<span class="number">10</span>])         <span class="comment"># y_</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))   <span class="comment"># 交叉熵, 符合上面的交叉熵公式 </span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy) <span class="comment"># 使用随机梯度下降算法最小化交叉熵</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()  <span class="comment"># sess 为 session, tf 中一切 Variable 的操作都需要用session 来控制</span></span><br><span class="line">sess.run(init)       <span class="comment"># 初始化后所有的 Variable 才能使用</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  <span class="comment"># 随机抓取 100 组数据作为一次训练的数据集. 再迭代训练 1000 次, 这样会利用上100_000组数据</span></span><br><span class="line">  batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>) </span><br><span class="line">  sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>)) <span class="comment"># 返回一组boolean值代表正确的预测</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>)) <span class="comment"># 将boolean值转为百分率</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) <span class="comment"># 输出正确率</span></span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个 MNIST 的项目是 tensorflow中文社区 介绍的深度学习领域中的 “Hellowrold”, 用来练手应该是非常合适的, 这也是我第一次使用tensorflow. 被它的强大所震撼.(上面的代码样例实际运行时间不会超过 3 秒钟). 而且这个项目代表了机器学习中一个十分经典的问题 – 多分类问题. </p>
<p>当读懂了教程后, 会发现整个项目的构建思路其实是无比清晰的, 从问题的数据集的数学表示, 到设计数学模型描述实际问题中数据的关系, 再到将数学模型用代码实现已经选择训练模型的策略和评估方法, 在思路上十分顺畅. 这种设计思路我觉得对将来我自己的项目十分有借鉴意义. 只可惜我的数学底子在浩瀚的数学公式面前显得实在是微不足道, 无法深入的探究数学公式的原理. 在这里立个flag, 寒假在学习深度学习和tensorflow时也要尽最大努力补充数学和概率论的知识, 只有这样才能从更底层的角度深入的理解深度学习.</p>
<h1 id="MINST卷积神经网络"><a href="#MINST卷积神经网络" class="headerlink" title="MINST卷积神经网络"></a>MINST卷积神经网络</h1><p>上面的MINST例子有91%的正确率, 下面将使用卷积神经网络来改善效果.</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>卷积网络(也叫做卷积神经网络), 是指那些至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络. 卷积是一种特殊的线性运算. 卷积网络是一种专门用来处理具有类似网络结构的数据的神经网络, 例如时间序列数据和二维的像素网络. </p>
<p>假设 x(t) 是某传感器传来的测量结果.<br>采用一个有效的概率密度函数(也算是一种加权函数)$W(a)$, a 表示距离某个特定自变量的距离大小, 对自变量的定义域的任意值均采用这种加权平均的操作, 就有<br>$$s(t) = \int x(a)w(t-a)da$$<br>这里 a 是当前时刻t之前的某个时刻. x 是输入(input), 第二个参数 w 叫做核函数. 输出有时被称作特征映射. 这种运算就叫做卷积. 用星号表示:<br>$$s(t) = (x * w)(t)$$<br>对于离散数据的二维卷积运算, 上述积分可以简化为<br>$$S(i,j)=(I * K)(i,j)=\sum_m \sum_nI(m,n)K(i-m,j-n)$$<br>因为卷积是可交换(翻转)的, 所以等价于<br>$$S(i,j)=(K * I)(i,j)=\sum_m \sum_nI(i-m,j-n)K(m,n)$$<br>许多神经网络库会实现一个相关的函数, 称为互相关函数, 和卷积运算一样但是没有对核翻转.<br>$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)$$<br>下面是个 2 维卷积的例子.</p>
<figure class="image-box">
                <a rel=机器学习入门 href="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/juanji.png" title="" data-fancybox="images"><img src="https://hexo-blog-1258787237.cos.ap-beijing.myqcloud.com/hexo-img/dachuang/juanji.png" alt title class></a>
                <p></p>
            </figure>

<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p><a href="https://zhuanlan.zhihu.com/p/35769417" target="_blank" rel="noopener">如何理解CNN中的池化？</a>.<br>我的通俗的理解: 池化用来提取一个张量中的最具有特征的值, 可能是最大值, 也可能是最小值或者平均值. 池化的意义在于可以在保持数据较少损失的同时减少数据规模, 降低运算成本.</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>下面要构建整个项目的网络结构. 总的来说分为 5 层, 分别是 第一层卷积运算, 第二层卷积运算, 第三层为全连接层, 第四层为 Dropout 层,最后一层为 softmax 输出层. 因为包含了卷积运算, 所以这个神经网络也是卷积神经网络. 这四层网络都有各自的作用.</p>
<h3 id="第一层卷积层"><a href="#第一层卷积层" class="headerlink" title="第一层卷积层."></a>第一层卷积层.</h3><p>第一层卷积=层直接对图片的原始数据进行处理, 作用是要对图片特征进行初步的提取. 在这层卷积中, 设定 filter(核) 的大小为 $5\times5$, 并每次提取出一个长度为 32 的向量.</p>
<p>为了符合tensorflow中卷积运算函数的格式要求, 将图片的张量形状设定为$x = [-1,28,28,1]$, 这里的 -1 代表未设定(因为图片数量不固定), 第二三个维度代表图片的长和宽, 第四个维度代表了颜色通道数, 因为数据集是灰度图, 所以只有一个维度, 如果是rgb图像那么就有三个维度. 可以把这个输入的 x 张量想象为 一摞叠起来的照片(具有厚度). 同时将 filter1 的张量形状设定为$[5,5,1,32]$, 这里第三个维度的 1 与图书数据的通道数对应. 我将其理解为长度为 1 的输入向量, 计算结果为长度为 32 的输出向量. 那么偏置量显然也要设定为长度为 32 的向量.</p>
<p>根据卷积运算的定义, 可以确定这里的 $result_1 = (x*filter1) + b$ 的结果是个形状为 $[28,28,32]$ 的张量.</p>
<p>但是这层卷积中还包括了池化. 这个项目中使用的是 <code>max_pool</code>, 直观的理解应该就是提取最明显的特征作为下一层的输入数据. 这里定义池化的模板大小为$2\times2$, 步长为 1, 边距为 0, 并且下面几层的池化均为这个设定. 这样每次池化会将相邻且不重叠的 $2\times2$ 个元素作为一个 padding, 提取出其中最”大”的数据保留, 其余数据删掉. 这里何为最大我不清楚, 有可能是向量的模长, 也可能是别的, 但是一定是最能反映图像特征的. 总之经过这样一层处理, 我们的数据先是被”挖掘”出更多的特征(卷积后的结果让数据长度由1变为32), 然后又经过”提炼”让数据量大幅变小(池化让图片每4个像素缩为一个, 最后成了$14\times14$的大小.)</p>
<p>$$[28,28,1] \underrightarrow{ 第一层卷积层 }[14,14,32]$$</p>
<p>没有图不好理解, 有空在网上找几个图补上.</p>
<h3 id="第二层卷积层"><a href="#第二层卷积层" class="headerlink" title="第二层卷积层"></a>第二层卷积层</h3><p>为了构建一个更深的网络, 将图片中的数据进一步提炼, 舍掉无用的数据, 这里再加入一层卷积层. 这一次要将32位的向量扩充到64位, 也就是计算出64个特征.</p>
<p>同样还是先卷积运算, 定义 filter2 的张量形式为 $[5,5,32,64]$, 显然偏置量就为一个64位的向量. </p>
<p>$result_2 = (result_1*filter2)$. </p>
<p>类似第一层, 还要在加上池化. 模板大小也为$2\times2$, 这样输出的图片就是$7\times7$了.</p>
<p>$$[14,14,32] \underrightarrow{ 第一层卷积层 }[7,7,64]$$</p>
<h3 id="第三层全连接层-密集连接层"><a href="#第三层全连接层-密集连接层" class="headerlink" title="第三层全连接层(密集连接层)"></a>第三层全连接层(密集连接层)</h3><p>全连接层的优点是每个输出的神经元都会受到输入数据的作用, 可以把输入数据的所有特征进行综合, 但是明显需要计算量也很大, 需要的参数也非常多. 所以之前两层也可以看做是为这层做准备, 一是要提前提取特征, 二是减少数据规模.  </p>
<p>经过前两层的处理, 数据规模已经足够小了, 且计算出了 64 个特征值. 全连接的输出应该为一个向量. 这里设定向量的长度为 1024, 所以权重张量设定为$[7,7,64,1024]$ 也就是说需要 1024 个结点(神经元)来计算. 那么可以算出实际全连接时需要的参数有$7\times7\times64\times1024$个, 已经是个很大的数字了, 如果之前数据规模没降下来, 那数字又会大的惊人.</p>
<p>同理偏置量也是1024位的向量.</p>
<figure class="image-box">
                <a rel=机器学习入门 href="https://img-blog.csdn.net/20170928110946345?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTAyMTc3Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" title="" data-fancybox="images"><img src="https://img-blog.csdn.net/20170928110946345?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTAyMTc3Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt title class></a>
                <p></p>
            </figure>


<p>$$result_3 = matmul(result_2, W) + b $$</p>
<p>$$[7,7,64] \underrightarrow{ 第三层全连接层 }[1024]$$</p>
<h3 id="第四层-Dropout-层"><a href="#第四层-Dropout-层" class="headerlink" title="第四层 Dropout 层"></a>第四层 Dropout 层</h3><p>这一层的目的是为了防止过拟合.</p>
<p><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
<p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图所示。</p>
<figure class="image-box">
                <a rel=机器学习入门 href="https://pic2.zhimg.com/v2-5530bdc5d49f9e261975521f8afd35e9_r.jpg" title="" data-fancybox="images"><img src="https://pic2.zhimg.com/v2-5530bdc5d49f9e261975521f8afd35e9_r.jpg" alt title class></a>
                <p></p>
            </figure>

<p>$$[1024] \underrightarrow{ 第四层Dropout层 }[1024]$$</p>
<h3 id="第五层输出层"><a href="#第五层输出层" class="headerlink" title="第五层输出层"></a>第五层输出层</h3><p>这一层和之前的基础教程没啥区别, 主要的作用还是通过softmax使神经网络的输出规范化为有意义的概率值. 也就是限定了网络输出的值域为[0,1], 同理输出为10位的向量, 代表了这个图像被分类到十个数字的概率.<br>$$[1024] \underrightarrow{ 第五层输出层 }[10]$$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"> </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(<span class="string">'float'</span>, shape=[<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_hat = tf.placeholder(<span class="string">'float'</span>, shape=[<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 第一层卷积</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"><span class="comment"># 第二层卷积</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"><span class="comment"># Dropout</span></span><br><span class="line">keep_prob = tf.placeholder(<span class="string">'float'</span>)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"><span class="comment"># softmax</span></span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"><span class="comment"># 训练评估模型</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_hat*tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_hat, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">'float'</span>))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>], y_hat: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_hat: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"> </span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_hat: mnist.test.labels, keep_prob: <span class="number">1.0</span></span><br><span class="line">&#125;))</span><br></pre></td></tr></table></figure>

<h1 id="tensorflow-常用-API"><a href="#tensorflow-常用-API" class="headerlink" title="tensorflow 常用 API"></a>tensorflow 常用 API</h1><h2 id="tf-version-1-x-2-x"><a href="#tf-version-1-x-2-x" class="headerlink" title="tf.* [version 1.x 2.x]"></a>tf.* [version 1.x 2.x]</h2><p>那些 tf 中的变量或method.</p>
<h3 id="method-trainable-variables-version-1-x"><a href="#method-trainable-variables-version-1-x" class="headerlink" title="method: trainable_variables[version 1.x]"></a>method: trainable_variables[version 1.x]</h3><p>Returns all variables created with trainable=True.<br>使用方法: <code>tvars = tf.trainable_variables()</code><br>tvars 中将会包含此程序中建立的所有可训练的参数.</p>
<h3 id="method-ones-like-version-1-x-2-x"><a href="#method-ones-like-version-1-x-2-x" class="headerlink" title="method: ones_like [version 1.x 2.x]"></a>method: ones_like [version 1.x 2.x]</h3><ul>
<li>函数原型: <code>tf.ones_like(tensor, dtype=None, name=None, optimize=True)</code></li>
<li>参数:<ul>
<li>tensor: A Tensor.</li>
<li>dtype: A type for the returned Tensor. Must be float32, float64, int8, uint8, int16, uint16, int32, int64, complex64, complex128 or bool.</li>
<li>name: A name for the operation (optional).</li>
<li>optimize: if true, attempt to statically determine the shape of ‘tensor’ and encode it as a constant.</li>
</ul>
</li>
<li>举例:<br>  <code>tensor = tf.constant([[1, 2, 3], [4, 5, 6]])</code><br>  <code>tf.ones_like(tensor)  # [[1, 1, 1], [1, 1, 1]]</code></li>
</ul>
<h3 id="method-zero-like-version-1-x-2-x"><a href="#method-zero-like-version-1-x-2-x" class="headerlink" title="method: zero_like [version 1.x 2.x]"></a>method: zero_like [version 1.x 2.x]</h3><p>类似 <code>tf.ones_like</code>, 具体略.</p>
<h3 id="method-reduce-mean-version-1-x"><a href="#method-reduce-mean-version-1-x" class="headerlink" title="method: reduce_mean [version 1.x]:"></a>method: reduce_mean [version 1.x]:</h3><p>tf.reduce_mean 函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。</p>
<ul>
<li>函数原型: <code>reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)</code></li>
<li>函数参数: <ul>
<li>input_tensor： 输入的待降维的tensor;</li>
<li>axis： 指定的轴，如果不指定，则计算所有元素的均值</li>
<li>keep_dims：是否降维度，设置为True，输出的结果保持输入tensor的形状，设置为False，输出结果会降低维度;</li>
<li>name： 操作的名称</li>
<li>reduction_indices：在以前版本中用来指定轴，已弃用;</li>
</ul>
</li>
</ul>
<h3 id="method-get-variable-version-1-x"><a href="#method-get-variable-version-1-x" class="headerlink" title="method: get_variable [version 1.x]"></a>method: get_variable [version 1.x]</h3><p>Gets an existing variable with these parameters or create a new one.<br>获取或新建一个拥有这些参数的变量. 被调用时, 此方法会以当前变量作用域名作为前缀, 并执行<code>reuse</code>检查.</p>
<ul>
<li><p>函数原型: <code>tf.get_variable(name,shape=None,dtype=None,initializer=None, regularizer=None, trainable=None, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None, constraint=None, synchronization=tf.VariableSynchronization.AUTO, aggregation=tf.VariableAggregation.NONE)</code></p>
</li>
<li><p>使用样例:</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    v = tf.get_variable(<span class="string">"v"</span>, [<span class="number">1</span>])</span><br><span class="line"><span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">v1 = foo()  <span class="comment"># Creates v.</span></span><br><span class="line">v2 = foo()  <span class="comment"># Gets the same, existing v.</span></span><br><span class="line"><span class="keyword">assert</span> v1 == v2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="tf-nn-version-1-x-2-x"><a href="#tf-nn-version-1-x-2-x" class="headerlink" title="tf.nn [version 1.x 2.x]"></a>tf.nn [version 1.x 2.x]</h2><p>Wrappers for primitive Neural Net (NN) Operations.<br>用于提供底层神经网络操作的支持. </p>
<h3 id="method-conv2d-version-1-x"><a href="#method-conv2d-version-1-x" class="headerlink" title="method: conv2d [version 1.x]"></a>method: conv2d [version 1.x]</h3><p>卷积运算.</p>
<ul>
<li>函数原型:<br>  <code>tf.nn.conv2d(input, filter=None, strides=None, padding=None, use_cudnn_on_gpu=True, data_format=&#39;NHWC&#39;, dilations=[1, 1, 1, 1], name=None, filters=None)</code></li>
</ul>
<h3 id="method-avg-pool-version-1-x-version-2-x"><a href="#method-avg-pool-version-1-x-version-2-x" class="headerlink" title="method: avg_pool [version 1.x, version 2.x]"></a>method: avg_pool [version 1.x, version 2.x]</h3><p>对输入信号执行平均池化操作.</p>
<ul>
<li>函数原型:<br>  <code>tf.nn.avg_pool(value, ksize, strides, padding, data_format=&#39;NHWC&#39;, name=None, input=None)</code></li>
<li>参数:<ul>
<li>value: A 4-D Tensor of shape [batch, height, width, channels] and type float32, float64, qint8, quint8, or qint32.</li>
<li>ksize: An int or list of ints that has length 1, 2 or 4. The size of the window for each dimension of the input tensor.</li>
<li>strides: An int or list of ints that has length 1, 2 or 4. The stride of + the sliding window for each dimension of the input tensor.</li>
<li>padding: A string, either ‘VALID’ or ‘SAME’. The padding algorithm. See + the “returns” section of tf.nn.convolution for details.</li>
<li>data_format: A string. ‘NHWC’ and ‘NCHW’ are supported.</li>
<li>name: Optional name for the operation.</li>
<li>input: Alias for value.</li>
</ul>
</li>
<li>使用例子:<br>  <code>d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)</code></li>
</ul>
<h3 id="relu-version-1-x"><a href="#relu-version-1-x" class="headerlink" title="relu [version 1.x]"></a>relu [version 1.x]</h3><p>relu, 全名为 rectified linear unit, 线性整流单元. 是一种常用的激励函数. 函数基本相当于: <code>lambda x: max(0, x)</code>.  </p>
<ul>
<li>函数原型: <code>tf.nn.relu(features, name=None)</code></li>
</ul>
<h3 id="sigmoid-cross-entropy-with-logits-version-1-x-2-x"><a href="#sigmoid-cross-entropy-with-logits-version-1-x-2-x" class="headerlink" title="sigmoid_cross_entropy_with_logits[version 1.x 2.x]"></a>sigmoid_cross_entropy_with_logits[version 1.x 2.x]</h3><p>Computes sigmoid cross entropy given logits<br>基于给定的参数计算 <code>sigmoid</code> 交叉熵.</p>
<ul>
<li>函数原型: <code>tf.nn.sigmoid_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, name=None)</code></li>
<li>函数参数: <ul>
<li>_sentinel: Used to prevent positional parameters. Internal, do not use. (用于避免传递位置参数, 不可使用)</li>
<li>labels: A Tensor of the same type and shape as logits.</li>
<li>logits: A Tensor of type float32 or float64. </li>
<li>name: A name for the operation (optional).</li>
</ul>
</li>
<li>例子: <ul>
<li><code>tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx))</code></li>
<li><code>tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg))</code></li>
</ul>
</li>
</ul>
<h2 id="tf-train-version-1-x"><a href="#tf-train-version-1-x" class="headerlink" title="tf.train [version 1.x]"></a>tf.train [version 1.x]</h2><h3 id="class-AdamOptimizer-version-1-x"><a href="#class-AdamOptimizer-version-1-x" class="headerlink" title="class: AdamOptimizer [version 1.x]"></a>class: AdamOptimizer [version 1.x]</h3><p>Optimizer that implements the Adam algorithm.<br>Adam优化器, 能按照 Adam 算法更新参数.</p>
<ul>
<li><p>数据域:</p>
<ul>
<li>loss: A Tensor containing the value to minimize.</li>
<li>global_step: Optional Variable to increment by one after the variables have been updated.</li>
<li>var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.</li>
<li>gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.</li>
<li>aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.</li>
<li>colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.</li>
<li>name: Optional name for the returned operation.</li>
<li>grad_loss: Optional. A Tensor holding the gradient computed for loss.</li>
</ul>
</li>
<li><p>method:</p>
<ul>
<li><code>__init__</code>:<br>  <code>__init__(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=&#39;Adam&#39;)</code></li>
<li><code>minimize</code>:<br>  <code>minimize(loss, global_step=None, var_list=None gate_gradients=GATE_OP, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)</code></li>
</ul>
</li>
<li><p>使用举例:<br>  <code>d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)</code><br>  <code>d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)</code></p>
</li>
</ul>
<h2 id="tf-summary"><a href="#tf-summary" class="headerlink" title="tf.summary []"></a>tf.summary []</h2><h3 id="scalar"><a href="#scalar" class="headerlink" title="scalar"></a>scalar</h3><h1 id="GAN入门"><a href="#GAN入门" class="headerlink" title="GAN入门"></a>GAN入门</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>一个 GAN 中包含两个模型: 生成模型 (Generator Model) 和一个判别模型(Discriminative Model)组成.<br>判别模型会接收两个输入, 分别为真实数据和假数据. 判别模型需要判断输入数据究竟是否为真. 生成模型通过反卷积神经网络将随机输入值转化为图像.<br>在训练过程中, 判别器学习从一堆生成器生成的假数字图像中, 找出真正的数字图像, 同时生成器通过判别器的反馈学习如何生成具有欺骗性的图片, 放着被判别器识别出来.<br>上述过程可以用公式描述:<br>$$<br>\underset{G}{min} \underset{D}{max}V(D,G) = E_{x\sim p_{data}(x)}[logD(x)] + E_{z\sim p_z(z)}[log(1-D(G(z)))]<br>$$</p>
<ul>
<li>其中 x 代表真实数据. 那么D(x)就是判别模型对真实数据的判别(认为真则为1, 否则为0). </li>
<li>z 代表外部输入的一个随机的噪声. 那么G(z)就是生成模型输出的伪造数据.</li>
<li>训练过程中, G总是试图让表达式右侧最小化, D总是试图让表达式右侧最大化, 由此两个网络产生对抗. </li>
</ul>

        </div>
        
<blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2020-02-11T13:16:58.419Z" itemprop="dateUpdated">2020-02-11 21:16:58</time>
</span><br>


        
        转载注明出处，原文地址：<a href="/2019/10/30/MNIST/" target="_blank" rel="external">https://tjuyjn.top/2019/10/30/MNIST/</a>
        
    </div>
    <footer>
        <a href="https://tjuyjn.top">
            <img src="/img/avatar.jpg" alt="Anti-entropy">
            Anti-entropy
        </a>
    </footer>
</blockquote>

        
            <div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>

            
        
        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>

            <div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://tjuyjn.top/2019/10/30/MNIST/&title=《机器学习入门》 — Anti-entropy's blog&pic=https://tjuyjn.top/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://tjuyjn.top/2019/10/30/MNIST/&title=《机器学习入门》 — Anti-entropy's blog&source=个人博客计算机" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://tjuyjn.top/2019/10/30/MNIST/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习入门》 — Anti-entropy's blog&url=https://tjuyjn.top/2019/10/30/MNIST/&via=https://tjuyjn.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://tjuyjn.top/2019/10/30/MNIST/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
      
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>

        </div>
        
            
    <div id="comment"></div>



        
    </div>
    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="prev">
      <a href="/2019/11/05/Computer-network-summary/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">计网 summary</h4>
      </a>
    </div>
  

  
    <div class="next">
      <a href="/2019/10/03/linux文件管理实验/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">一些有趣的实验</h4>
      </a>
    </div>
  
</nav>


    
    
        <aside class="post-widget">
            <nav class="post-toc-wrap" id="post-toc">
                <strong>目录</strong>
                <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#MNIST神经网络"><span class="post-toc-number">1.</span> <span class="post-toc-text">MNIST神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#简介"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">简介</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数据集"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">数据集</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#softmax回归"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">softmax回归</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型设定"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">模型设定</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练模型"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">训练模型</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型评估"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">模型评估</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#完整代码和我的注释"><span class="post-toc-number">1.7.</span> <span class="post-toc-text">完整代码和我的注释</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总结"><span class="post-toc-number">1.8.</span> <span class="post-toc-text">总结</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#MINST卷积神经网络"><span class="post-toc-number">2.</span> <span class="post-toc-text">MINST卷积神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷积"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">卷积</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#池化"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">池化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷积神经网络"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">卷积神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#第一层卷积层"><span class="post-toc-number">2.3.1.</span> <span class="post-toc-text">第一层卷积层.</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#第二层卷积层"><span class="post-toc-number">2.3.2.</span> <span class="post-toc-text">第二层卷积层</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#第三层全连接层-密集连接层"><span class="post-toc-number">2.3.3.</span> <span class="post-toc-text">第三层全连接层(密集连接层)</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#第四层-Dropout-层"><span class="post-toc-number">2.3.4.</span> <span class="post-toc-text">第四层 Dropout 层</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#第五层输出层"><span class="post-toc-number">2.3.5.</span> <span class="post-toc-text">第五层输出层</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#代码"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">代码</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tensorflow-常用-API"><span class="post-toc-number">3.</span> <span class="post-toc-text">tensorflow 常用 API</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-version-1-x-2-x"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">tf.* [version 1.x 2.x]</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-trainable-variables-version-1-x"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">method: trainable_variables[version 1.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-ones-like-version-1-x-2-x"><span class="post-toc-number">3.1.2.</span> <span class="post-toc-text">method: ones_like [version 1.x 2.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-zero-like-version-1-x-2-x"><span class="post-toc-number">3.1.3.</span> <span class="post-toc-text">method: zero_like [version 1.x 2.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-reduce-mean-version-1-x"><span class="post-toc-number">3.1.4.</span> <span class="post-toc-text">method: reduce_mean [version 1.x]:</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-get-variable-version-1-x"><span class="post-toc-number">3.1.5.</span> <span class="post-toc-text">method: get_variable [version 1.x]</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-nn-version-1-x-2-x"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">tf.nn [version 1.x 2.x]</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-conv2d-version-1-x"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">method: conv2d [version 1.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#method-avg-pool-version-1-x-version-2-x"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">method: avg_pool [version 1.x, version 2.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#relu-version-1-x"><span class="post-toc-number">3.2.3.</span> <span class="post-toc-text">relu [version 1.x]</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#sigmoid-cross-entropy-with-logits-version-1-x-2-x"><span class="post-toc-number">3.2.4.</span> <span class="post-toc-text">sigmoid_cross_entropy_with_logits[version 1.x 2.x]</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-train-version-1-x"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">tf.train [version 1.x]</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#class-AdamOptimizer-version-1-x"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">class: AdamOptimizer [version 1.x]</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tf-summary"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">tf.summary []</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#scalar"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">scalar</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#GAN入门"><span class="post-toc-number">4.</span> <span class="post-toc-text">GAN入门</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本原理"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">基本原理</span></a></li></ol></li></ol>
            </nav>
            <div class="toc-bar"><div>
        </aside>
    
</article>

    <div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        我们一起来让这个世界有趣一点
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/reward-wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/reward-wechat.jpg" data-alipay="/img/reward-alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>

    

</div>

    </main>
    <footer class="footer ">
    
    <div class="top">
        
            <p>
    <span>Links：</span>
    
    <span class="blogroll-item">
        <a href="https://www.lujingtao.com" target="_blank">主题作者</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://www.tjuscswyz.cn/" target="_blank">p4ssw0rd</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://www.cnblogs.com/ever17/" target="_blank">Ever17</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://www.cnblogs.com/tkj521Ya/" target="_blank">Exaggerate~</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://r1895.github.io" target="_blank">jason1999</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://tjublesson.top/" target="_blank">blesson</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://glimmerzcy.github.io" target="_blank">ZCY</a>
    </span>
    
    <span class="blogroll-item">
        <a href="https://blog.csdn.net/Harry____" target="_blank">Harry____</a>
    </span>
    
    <span class="blogroll-item">
        <a href="http://daemon.tjunsa.com/" target="_blank">deamon</a>
    </span>
    
</p>

        
    </div>
    
    <div class="bottom">
        <p>
            <span>
                Anti-entropy &copy; 2019 - 2020
            </span>
        		
           	
            
            
            
            

            
                
<span class="site-uv" title="总访客量">
    <i class="icon icon-user"></i>
    <i class="busuanzi-value" id="busuanzi_value_site_uv"></i>
</span>


<span class="site-pv" title="总访问量">
    <i class="icon icon-eye"></i>
    <i class="busuanzi-value" id="busuanzi_value_site_pv"></i>
</span>

            
        </p>
    </div>
</footer>

    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://tjuyjn.top/2019/10/30/MNIST/&title=《机器学习入门》 — Anti-entropy's blog&pic=https://tjuyjn.top/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://tjuyjn.top/2019/10/30/MNIST/&title=《机器学习入门》 — Anti-entropy's blog&source=个人博客计算机" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://tjuyjn.top/2019/10/30/MNIST/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习入门》 — Anti-entropy's blog&url=https://tjuyjn.top/2019/10/30/MNIST/&via=https://tjuyjn.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://tjuyjn.top/2019/10/30/MNIST/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
      
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAAB0ElEQVR42u3aQY7DIAwF0Nz/0h1pVjOqSr4xoFR6rKokDY8sLGxzXfF4/Y7333+vvD+TXLl2DFxc3Db3NRzjZ/K7+fI+vQcXF/c8Nwle4wnG15N/jQ24uLjP5CbBaLzU5HlcXNxv5N5sQYZvxsXF/S5unsDk0yShcGOuhouL2+B2Cqarfh+q7+Li4ra7EtWtzDhUzc347824uLhHuHNFzGrgq94dfwJcXNzd3Oqxibnm6LLPgYuLe4SbgMoFzbjwkW96cHFxT3LnChnVK9X2bdTzwcXF3caNcqNiobPDLWyGcHFxt3ETYrVd2vpmk4UdXFzcXXWG6gKqC6sWXAr7Mlxc3A3c6iGJfokkb7u2zoPg4uJOcfOjmZ1EKA+a5eQHFxd3A3dVIMujZmepuLi4Z7ido5b9OkYy+83BLFxc3M3c6qhuU5L85aY0g4uLe4RbDUBzRdJrapTRuLi4i7hzwauaj+SHNW9CIS4u7hHuqv1DZ7vTac/g4uLu43aSnLxxkrRXo70YLi7uQW61uVJtnCwLnbi4uA/j9gNTfvemyouLi/swbr6Mask1Cny4uLgHudVwU22EVNOnZbkaLi7uFLeaiuQLyFss1SMauLi427g/b8jDQjwhLHsAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>


    
    <!-- main-js -->
<script type="text/javascript" src="//cdn.bootcss.com/jquery/2.1.0/jquery.min.js"></script>
<script type="text/javascript" src="/js/plugins/fastclick.js?v=1.4.2"></script>
<script type="text/javascript" src="/js/plugins/ios-orientationchange-fix.js?v=1.4.2"></script>
<script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.js"></script>

<script type="text/javascript" src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>

<script type="text/javascript" src="/js/method.js?v=1.4.2"></script>
<script type="text/javascript" src="/js/blog.js?v=1.4.2"></script>

<!-- third-party -->






<script type="text/javascript" src="/js/plugins/local_search.js?v=1.4.2"></script>
<script type="text/javascript">
	var search_path = "search.xml";
	if (search_path.length === 0) {
		search_path = "search.xml";
	}
	var path = "/" + search_path;
	searchFunc(path, "local-search-input", "local-search-result");
</script>



<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



    
        <script type="text/javascript" src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script type="text/javascript" src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script type="text/javascript" src="/js/plugins/valine.js?v=1.4.2"></script>
    
    





    <!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>




    <script>
    (function() {
        var OriginTitile = document.title, titleTime;
        document.addEventListener('visibilitychange', function() {
            if (document.hidden) {
                document.title = '不要离开我！';
                clearTimeout(titleTime);
            } else {
                document.title = '嘤嘤嘤!太好了~';
                titleTime = setTimeout(function() {
                    document.title = OriginTitile;
                },2000);
            }
        });
    })();
</script>





    
    <script type="text/javascript">           
        /* 鼠标特效 */ 
        var a_idx = 0; 
        jQuery(document).ready(function($) { 
            $("body").click(function(e) { 
                var a = new Array("❤好❤","❤看❤","❤就❤","❤留❤","❤个❤","❤言❤","❤吧❤"); 
                var $i = $("<span></span>").text(a[a_idx]); 
                a_idx = (a_idx + 1) % a.length; 
                var x = e.pageX, 
                y = e.pageY; 
                $i.css({ 
                    "z-index": 999999999999999999999999999999999999999999999999999999999999999999999, 
                    "top": y - 20, 
                    "left": x, 
                    "position": "absolute", 
                    "font-weight": "bold", 
                    "color": "rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")" 
                }); 
                $("body").append($i); 
                $i.animate({ 
                    "top": y - 180, 
                    "opacity": 0 
                }, 
                1500, 
                function() { 
                    $i.remove(); 
                }); 
            }); 
        }); 
    </script>
    <!--动态线条背景-->
    <script type="text/javascript" color="30,144,255" opacity='0.7' zIndex="-2" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


</body>
</html>
